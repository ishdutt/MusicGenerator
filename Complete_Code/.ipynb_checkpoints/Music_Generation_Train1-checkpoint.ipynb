{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Problem\n",
    "\n",
    "This case-study focuses on generating music automatically using Recurrent Neural Network(RNN).<br> \n",
    "We do not necessarily have to be a music expert in order to generate music. Even a non expert can generate a decent quality music using RNN.<br>\n",
    "We all like to listen interesting music and if there is some way to generate music automatically, particularly decent quality music then it's a big leap in the world of music industry.<br><br>\n",
    "<b>Task:</b> Our task here is to take some existing music data then train a model using this existing data. The model has to learn the patterns in music that we humans enjoy. Once it learns this, the model should be able to generate new music for us. It cannot simply copy-paste from the training data. It has to understand the patterns of music to generate new music. We here are not expecting our model to generate new music which is of professional quality, but we want it to generate a decent quality music which should be melodious and good to hear.<br><br>\n",
    "Now, what is music? In short music is nothing but a sequence of musical notes. Our input to the model is a sequence of musical events/notes. Our output will be new sequence of musical events/notes. In this case-study we have limited our self to single instrument music as this is our first cut model. In future, we will extend this to multiple instrument music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source:\n",
    "1. http://abc.sourceforge.net/NMD/\n",
    "2. http://trillian.mit.edu/~jc/music/book/oneills/1850/X/\n",
    "\n",
    "### From first data-source, we have downloaded first two files:\n",
    "* Jigs (340 tunes)\n",
    "* Hornpipes (65 tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../Data/\"\n",
    "data_file = \"Data_Tunes.txt\"\n",
    "charIndex_json = \"char_to_index.json\"\n",
    "model_weights_directory = '../Data/Model_Weights/'\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(all_chars, unique_chars):\n",
    "    length = all_chars.shape[0]\n",
    "    batch_chars = int(length / BATCH_SIZE) #155222/16 = 9701\n",
    "    \n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, 64):  #(0, 9637, 64)  #it denotes number of batches. It runs everytime when\n",
    "        #new batch is created. We have a total of 151 batches.\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    #(16, 64)\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))   #(16, 64, 87)\n",
    "        for batch_index in range(0, 16):  #it denotes each row in a batch.  \n",
    "            for i in range(0, 64):  #it denotes each column in a batch. Each column represents each character means \n",
    "                #each time-step character in a sequence.\n",
    "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
    "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1 #here we have added '1' because the\n",
    "                #correct label will be the next character in the sequence. So, the next character will be denoted by\n",
    "                #all_chars[batch_index * batch_chars + start + i + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(batch_size, seq_length, unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(128, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(data, epochs = 80):\n",
    "    #mapping character to index\n",
    "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Number of unique characters in our whole tunes database = {}\".format(len(char_to_index))) #87\n",
    "    \n",
    "    with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
    "        json.dump(char_to_index, f)\n",
    "        \n",
    "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
    "    unique_chars = len(char_to_index)\n",
    "    \n",
    "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
    "    print(\"Total number of characters = \"+str(all_characters.shape[0])) #155222\n",
    "    \n",
    "    epoch_number, loss, accuracy = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
    "        epoch_number.append(epoch+1)\n",
    "        \n",
    "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
    "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
    "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
    "        loss.append(final_epoch_loss)\n",
    "        accuracy.append(final_epoch_accuracy)\n",
    "        \n",
    "        #saving weights after every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(model_weights_directory):\n",
    "                os.makedirs(model_weights_directory)\n",
    "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1)))\n",
    "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
    "    \n",
    "    #creating dataframe and record all the losses and accuracies at each epoch\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = epoch_number\n",
    "    log_frame[\"Loss\"] = loss\n",
    "    log_frame[\"Accuracy\"] = accuracy\n",
    "    log_frame.to_csv(\"../Data/log.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in our whole tunes database = 87\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (16, 64, 512)             44544     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (16, 64, 128)             197120    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (16, 64, 128)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (16, 64, 87)              11223     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (16, 64, 87)              0         \n",
      "=================================================================\n",
      "Total params: 1,040,343\n",
      "Trainable params: 1,040,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total number of characters = 155222\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishdutt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss: 4.46358585357666, Accuracy: 0.0244140625\n",
      "Batch: 2, Loss: 4.436729907989502, Accuracy: 0.17578125\n",
      "Batch: 3, Loss: 4.403998374938965, Accuracy: 0.134765625\n",
      "Batch: 4, Loss: 4.350777626037598, Accuracy: 0.1025390625\n",
      "Batch: 5, Loss: 4.109460830688477, Accuracy: 0.1435546875\n",
      "Batch: 6, Loss: 3.847993850708008, Accuracy: 0.166015625\n",
      "Batch: 7, Loss: 3.7958757877349854, Accuracy: 0.1630859375\n",
      "Batch: 8, Loss: 3.784571409225464, Accuracy: 0.142578125\n",
      "Batch: 9, Loss: 3.7929766178131104, Accuracy: 0.13671875\n",
      "Batch: 10, Loss: 3.6339097023010254, Accuracy: 0.1484375\n",
      "Batch: 11, Loss: 3.431927442550659, Accuracy: 0.15234375\n",
      "Batch: 12, Loss: 3.5604872703552246, Accuracy: 0.123046875\n",
      "Batch: 13, Loss: 3.766331672668457, Accuracy: 0.1162109375\n",
      "Batch: 14, Loss: 3.5170960426330566, Accuracy: 0.1455078125\n",
      "Batch: 15, Loss: 3.7265844345092773, Accuracy: 0.1298828125\n",
      "Batch: 16, Loss: 3.4580650329589844, Accuracy: 0.1572265625\n",
      "Batch: 17, Loss: 3.3670132160186768, Accuracy: 0.1767578125\n",
      "Batch: 18, Loss: 3.354898452758789, Accuracy: 0.173828125\n",
      "Batch: 19, Loss: 3.628655433654785, Accuracy: 0.1298828125\n",
      "Batch: 20, Loss: 3.700364112854004, Accuracy: 0.109375\n",
      "Batch: 21, Loss: 3.576378107070923, Accuracy: 0.125\n",
      "Batch: 22, Loss: 3.3202457427978516, Accuracy: 0.1650390625\n",
      "Batch: 23, Loss: 3.435122013092041, Accuracy: 0.1357421875\n",
      "Batch: 24, Loss: 3.6073482036590576, Accuracy: 0.103515625\n",
      "Batch: 25, Loss: 3.5041356086730957, Accuracy: 0.1328125\n",
      "Batch: 26, Loss: 3.484297513961792, Accuracy: 0.126953125\n",
      "Batch: 27, Loss: 3.456220865249634, Accuracy: 0.1337890625\n",
      "Batch: 28, Loss: 3.2957963943481445, Accuracy: 0.1533203125\n",
      "Batch: 29, Loss: 3.479982376098633, Accuracy: 0.1259765625\n",
      "Batch: 30, Loss: 3.802619695663452, Accuracy: 0.087890625\n",
      "Batch: 31, Loss: 3.6863646507263184, Accuracy: 0.1142578125\n",
      "Batch: 32, Loss: 3.4361319541931152, Accuracy: 0.130859375\n",
      "Batch: 33, Loss: 3.433464527130127, Accuracy: 0.1494140625\n",
      "Batch: 34, Loss: 3.4217588901519775, Accuracy: 0.142578125\n",
      "Batch: 35, Loss: 3.5056257247924805, Accuracy: 0.1171875\n",
      "Batch: 36, Loss: 3.6450862884521484, Accuracy: 0.0966796875\n",
      "Batch: 37, Loss: 3.4993629455566406, Accuracy: 0.1162109375\n",
      "Batch: 38, Loss: 3.4160685539245605, Accuracy: 0.134765625\n",
      "Batch: 39, Loss: 3.486456871032715, Accuracy: 0.1259765625\n",
      "Batch: 40, Loss: 3.615328311920166, Accuracy: 0.1103515625\n",
      "Batch: 41, Loss: 3.5881118774414062, Accuracy: 0.1259765625\n",
      "Batch: 42, Loss: 3.4650015830993652, Accuracy: 0.142578125\n",
      "Batch: 43, Loss: 3.3227264881134033, Accuracy: 0.16015625\n",
      "Batch: 44, Loss: 3.332376480102539, Accuracy: 0.166015625\n",
      "Batch: 45, Loss: 3.358581781387329, Accuracy: 0.15234375\n",
      "Batch: 46, Loss: 3.660964012145996, Accuracy: 0.111328125\n",
      "Batch: 47, Loss: 3.6435632705688477, Accuracy: 0.103515625\n",
      "Batch: 48, Loss: 3.4373574256896973, Accuracy: 0.1376953125\n",
      "Batch: 49, Loss: 3.3914453983306885, Accuracy: 0.1357421875\n",
      "Batch: 50, Loss: 3.3591129779815674, Accuracy: 0.142578125\n",
      "Batch: 51, Loss: 3.358827829360962, Accuracy: 0.140625\n",
      "Batch: 52, Loss: 3.450453519821167, Accuracy: 0.1201171875\n",
      "Batch: 53, Loss: 3.4409403800964355, Accuracy: 0.1240234375\n",
      "Batch: 54, Loss: 3.5051136016845703, Accuracy: 0.130859375\n",
      "Batch: 55, Loss: 3.406245231628418, Accuracy: 0.13671875\n",
      "Batch: 56, Loss: 3.438364267349243, Accuracy: 0.1533203125\n",
      "Batch: 57, Loss: 3.4758572578430176, Accuracy: 0.12890625\n",
      "Batch: 58, Loss: 3.3494081497192383, Accuracy: 0.1416015625\n",
      "Batch: 59, Loss: 3.639573574066162, Accuracy: 0.115234375\n",
      "Batch: 60, Loss: 3.417316436767578, Accuracy: 0.1416015625\n",
      "Batch: 61, Loss: 3.4004080295562744, Accuracy: 0.1494140625\n",
      "Batch: 62, Loss: 3.4886646270751953, Accuracy: 0.1357421875\n",
      "Batch: 63, Loss: 3.4205987453460693, Accuracy: 0.1396484375\n",
      "Batch: 64, Loss: 3.4675145149230957, Accuracy: 0.142578125\n",
      "Batch: 65, Loss: 3.4195733070373535, Accuracy: 0.146484375\n",
      "Batch: 66, Loss: 3.404048442840576, Accuracy: 0.150390625\n",
      "Batch: 67, Loss: 3.2601394653320312, Accuracy: 0.1748046875\n",
      "Batch: 68, Loss: 3.330867052078247, Accuracy: 0.185546875\n",
      "Batch: 69, Loss: 3.4004735946655273, Accuracy: 0.1494140625\n",
      "Batch: 70, Loss: 3.379445791244507, Accuracy: 0.169921875\n",
      "Batch: 71, Loss: 3.289340019226074, Accuracy: 0.16796875\n",
      "Batch: 72, Loss: 3.2947874069213867, Accuracy: 0.166015625\n",
      "Batch: 73, Loss: 3.420726776123047, Accuracy: 0.1513671875\n",
      "Batch: 74, Loss: 3.3627371788024902, Accuracy: 0.1513671875\n",
      "Batch: 75, Loss: 3.186206340789795, Accuracy: 0.1748046875\n",
      "Batch: 76, Loss: 3.080202579498291, Accuracy: 0.1953125\n",
      "Batch: 77, Loss: 3.18571138381958, Accuracy: 0.1923828125\n",
      "Batch: 78, Loss: 3.658949851989746, Accuracy: 0.130859375\n",
      "Batch: 79, Loss: 3.6032488346099854, Accuracy: 0.1396484375\n",
      "Batch: 80, Loss: 3.2081401348114014, Accuracy: 0.20703125\n",
      "Batch: 81, Loss: 3.047745704650879, Accuracy: 0.240234375\n",
      "Batch: 82, Loss: 3.1512451171875, Accuracy: 0.2080078125\n",
      "Batch: 83, Loss: 3.1984212398529053, Accuracy: 0.203125\n",
      "Batch: 84, Loss: 3.324148654937744, Accuracy: 0.171875\n",
      "Batch: 85, Loss: 3.286198139190674, Accuracy: 0.1591796875\n",
      "Batch: 86, Loss: 3.1111440658569336, Accuracy: 0.2099609375\n",
      "Batch: 87, Loss: 3.1668925285339355, Accuracy: 0.2060546875\n",
      "Batch: 88, Loss: 3.091162919998169, Accuracy: 0.208984375\n",
      "Batch: 89, Loss: 3.107691526412964, Accuracy: 0.2138671875\n",
      "Batch: 90, Loss: 3.1944775581359863, Accuracy: 0.19921875\n",
      "Batch: 91, Loss: 3.1802797317504883, Accuracy: 0.1923828125\n",
      "Batch: 92, Loss: 3.0858592987060547, Accuracy: 0.2158203125\n",
      "Batch: 93, Loss: 3.0854508876800537, Accuracy: 0.2099609375\n",
      "Batch: 94, Loss: 2.9901931285858154, Accuracy: 0.248046875\n",
      "Batch: 95, Loss: 2.84053897857666, Accuracy: 0.267578125\n",
      "Batch: 96, Loss: 3.1157262325286865, Accuracy: 0.2216796875\n",
      "Batch: 97, Loss: 3.094085931777954, Accuracy: 0.224609375\n",
      "Batch: 98, Loss: 3.1109390258789062, Accuracy: 0.2119140625\n",
      "Batch: 99, Loss: 2.907475471496582, Accuracy: 0.2509765625\n",
      "Batch: 100, Loss: 2.782823085784912, Accuracy: 0.2529296875\n",
      "Batch: 101, Loss: 2.9059340953826904, Accuracy: 0.2392578125\n",
      "Batch: 102, Loss: 2.8985893726348877, Accuracy: 0.2451171875\n",
      "Batch: 103, Loss: 3.117239475250244, Accuracy: 0.2138671875\n",
      "Batch: 104, Loss: 2.8700385093688965, Accuracy: 0.236328125\n",
      "Batch: 105, Loss: 2.85294246673584, Accuracy: 0.2529296875\n",
      "Batch: 106, Loss: 2.9461472034454346, Accuracy: 0.2431640625\n",
      "Batch: 107, Loss: 2.9418423175811768, Accuracy: 0.2509765625\n",
      "Batch: 108, Loss: 2.953155040740967, Accuracy: 0.244140625\n",
      "Batch: 109, Loss: 2.839298725128174, Accuracy: 0.248046875\n",
      "Batch: 110, Loss: 2.8206934928894043, Accuracy: 0.2529296875\n",
      "Batch: 111, Loss: 2.7396910190582275, Accuracy: 0.2744140625\n",
      "Batch: 112, Loss: 2.975642442703247, Accuracy: 0.2412109375\n",
      "Batch: 113, Loss: 2.9752187728881836, Accuracy: 0.240234375\n",
      "Batch: 114, Loss: 2.76116681098938, Accuracy: 0.2841796875\n",
      "Batch: 115, Loss: 2.8248682022094727, Accuracy: 0.2734375\n",
      "Batch: 116, Loss: 2.8894309997558594, Accuracy: 0.251953125\n",
      "Batch: 117, Loss: 2.994887351989746, Accuracy: 0.2431640625\n",
      "Batch: 118, Loss: 2.9751055240631104, Accuracy: 0.240234375\n",
      "Batch: 119, Loss: 2.954641819000244, Accuracy: 0.2509765625\n",
      "Batch: 120, Loss: 2.721500873565674, Accuracy: 0.2861328125\n",
      "Batch: 121, Loss: 2.7450008392333984, Accuracy: 0.2978515625\n",
      "Batch: 122, Loss: 2.999901294708252, Accuracy: 0.2294921875\n",
      "Batch: 123, Loss: 2.9344186782836914, Accuracy: 0.2412109375\n",
      "Batch: 124, Loss: 2.8868536949157715, Accuracy: 0.265625\n",
      "Batch: 125, Loss: 2.7446537017822266, Accuracy: 0.28125\n",
      "Batch: 126, Loss: 2.7161383628845215, Accuracy: 0.2646484375\n",
      "Batch: 127, Loss: 2.9031381607055664, Accuracy: 0.240234375\n",
      "Batch: 128, Loss: 2.857313632965088, Accuracy: 0.2626953125\n",
      "Batch: 129, Loss: 2.7989914417266846, Accuracy: 0.265625\n",
      "Batch: 130, Loss: 2.8012478351593018, Accuracy: 0.2744140625\n",
      "Batch: 131, Loss: 2.8344831466674805, Accuracy: 0.26171875\n",
      "Batch: 132, Loss: 2.9006221294403076, Accuracy: 0.2646484375\n",
      "Batch: 133, Loss: 2.762739658355713, Accuracy: 0.2734375\n",
      "Batch: 134, Loss: 2.6987390518188477, Accuracy: 0.2890625\n",
      "Batch: 135, Loss: 2.630687713623047, Accuracy: 0.302734375\n",
      "Batch: 136, Loss: 2.6094400882720947, Accuracy: 0.3076171875\n",
      "Batch: 137, Loss: 2.463498115539551, Accuracy: 0.34375\n",
      "Batch: 138, Loss: 2.5695853233337402, Accuracy: 0.337890625\n",
      "Batch: 139, Loss: 2.56777286529541, Accuracy: 0.314453125\n",
      "Batch: 140, Loss: 2.662929058074951, Accuracy: 0.291015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 141, Loss: 2.682910919189453, Accuracy: 0.302734375\n",
      "Batch: 142, Loss: 2.6132988929748535, Accuracy: 0.3076171875\n",
      "Batch: 143, Loss: 2.7360193729400635, Accuracy: 0.2841796875\n",
      "Batch: 144, Loss: 2.7229232788085938, Accuracy: 0.29296875\n",
      "Batch: 145, Loss: 2.580122709274292, Accuracy: 0.3076171875\n",
      "Batch: 146, Loss: 2.685756206512451, Accuracy: 0.2802734375\n",
      "Batch: 147, Loss: 2.6744112968444824, Accuracy: 0.28515625\n",
      "Batch: 148, Loss: 2.5625901222229004, Accuracy: 0.30078125\n",
      "Batch: 149, Loss: 2.6719813346862793, Accuracy: 0.28515625\n",
      "Batch: 150, Loss: 2.6039376258850098, Accuracy: 0.3017578125\n",
      "Batch: 151, Loss: 2.7212376594543457, Accuracy: 0.28515625\n",
      "Epoch 2/80\n",
      "Batch: 1, Loss: 2.451472043991089, Accuracy: 0.32421875\n",
      "Batch: 2, Loss: 2.351924180984497, Accuracy: 0.337890625\n",
      "Batch: 3, Loss: 2.5833945274353027, Accuracy: 0.31640625\n",
      "Batch: 4, Loss: 2.6778080463409424, Accuracy: 0.294921875\n",
      "Batch: 5, Loss: 2.5320940017700195, Accuracy: 0.33203125\n",
      "Batch: 6, Loss: 2.3889882564544678, Accuracy: 0.3271484375\n",
      "Batch: 7, Loss: 2.403703451156616, Accuracy: 0.3359375\n",
      "Batch: 8, Loss: 2.472212314605713, Accuracy: 0.337890625\n",
      "Batch: 9, Loss: 2.511093854904175, Accuracy: 0.330078125\n",
      "Batch: 10, Loss: 2.42210054397583, Accuracy: 0.341796875\n",
      "Batch: 11, Loss: 2.254314422607422, Accuracy: 0.3701171875\n",
      "Batch: 12, Loss: 2.461894989013672, Accuracy: 0.3251953125\n",
      "Batch: 13, Loss: 2.5604543685913086, Accuracy: 0.3203125\n",
      "Batch: 14, Loss: 2.455069065093994, Accuracy: 0.3486328125\n",
      "Batch: 15, Loss: 2.6210269927978516, Accuracy: 0.310546875\n",
      "Batch: 16, Loss: 2.3862617015838623, Accuracy: 0.3486328125\n",
      "Batch: 17, Loss: 2.39454984664917, Accuracy: 0.3564453125\n",
      "Batch: 18, Loss: 2.3618364334106445, Accuracy: 0.33984375\n",
      "Batch: 19, Loss: 2.5287842750549316, Accuracy: 0.3271484375\n",
      "Batch: 20, Loss: 2.588322639465332, Accuracy: 0.3154296875\n",
      "Batch: 21, Loss: 2.428354024887085, Accuracy: 0.3427734375\n",
      "Batch: 22, Loss: 2.3599629402160645, Accuracy: 0.35546875\n",
      "Batch: 23, Loss: 2.3321382999420166, Accuracy: 0.3603515625\n",
      "Batch: 24, Loss: 2.5228254795074463, Accuracy: 0.3271484375\n",
      "Batch: 25, Loss: 2.3895998001098633, Accuracy: 0.337890625\n",
      "Batch: 26, Loss: 2.3384642601013184, Accuracy: 0.369140625\n",
      "Batch: 27, Loss: 2.419576644897461, Accuracy: 0.337890625\n",
      "Batch: 28, Loss: 2.2929861545562744, Accuracy: 0.369140625\n",
      "Batch: 29, Loss: 2.3798112869262695, Accuracy: 0.3388671875\n",
      "Batch: 30, Loss: 2.584737777709961, Accuracy: 0.3212890625\n",
      "Batch: 31, Loss: 2.574625015258789, Accuracy: 0.326171875\n",
      "Batch: 32, Loss: 2.339334487915039, Accuracy: 0.376953125\n",
      "Batch: 33, Loss: 2.4170961380004883, Accuracy: 0.361328125\n",
      "Batch: 34, Loss: 2.445979595184326, Accuracy: 0.33984375\n",
      "Batch: 35, Loss: 2.4193692207336426, Accuracy: 0.349609375\n",
      "Batch: 36, Loss: 2.544985294342041, Accuracy: 0.333984375\n",
      "Batch: 37, Loss: 2.4413065910339355, Accuracy: 0.33984375\n",
      "Batch: 38, Loss: 2.345890998840332, Accuracy: 0.3583984375\n",
      "Batch: 39, Loss: 2.4151601791381836, Accuracy: 0.33984375\n",
      "Batch: 40, Loss: 2.471740245819092, Accuracy: 0.3828125\n",
      "Batch: 41, Loss: 2.4007396697998047, Accuracy: 0.3662109375\n",
      "Batch: 42, Loss: 2.206477642059326, Accuracy: 0.396484375\n",
      "Batch: 43, Loss: 2.1486620903015137, Accuracy: 0.4033203125\n",
      "Batch: 44, Loss: 2.151242971420288, Accuracy: 0.4091796875\n",
      "Batch: 45, Loss: 2.146972179412842, Accuracy: 0.427734375\n",
      "Batch: 46, Loss: 2.400839328765869, Accuracy: 0.3720703125\n",
      "Batch: 47, Loss: 2.4324288368225098, Accuracy: 0.3720703125\n",
      "Batch: 48, Loss: 2.3877413272857666, Accuracy: 0.365234375\n",
      "Batch: 49, Loss: 2.3517374992370605, Accuracy: 0.361328125\n",
      "Batch: 50, Loss: 2.3420374393463135, Accuracy: 0.3720703125\n",
      "Batch: 51, Loss: 2.3520164489746094, Accuracy: 0.37109375\n",
      "Batch: 52, Loss: 2.3800010681152344, Accuracy: 0.3779296875\n",
      "Batch: 53, Loss: 2.2151095867156982, Accuracy: 0.40234375\n",
      "Batch: 54, Loss: 2.3111443519592285, Accuracy: 0.3994140625\n",
      "Batch: 55, Loss: 2.1761109828948975, Accuracy: 0.4052734375\n",
      "Batch: 56, Loss: 2.3175649642944336, Accuracy: 0.3720703125\n",
      "Batch: 57, Loss: 2.266491413116455, Accuracy: 0.388671875\n",
      "Batch: 58, Loss: 2.2974157333374023, Accuracy: 0.3837890625\n",
      "Batch: 59, Loss: 2.2418510913848877, Accuracy: 0.421875\n",
      "Batch: 60, Loss: 2.1249661445617676, Accuracy: 0.4169921875\n",
      "Batch: 61, Loss: 2.159890651702881, Accuracy: 0.4150390625\n",
      "Batch: 62, Loss: 2.3187265396118164, Accuracy: 0.3876953125\n",
      "Batch: 63, Loss: 2.1744627952575684, Accuracy: 0.3916015625\n",
      "Batch: 64, Loss: 2.1871962547302246, Accuracy: 0.408203125\n",
      "Batch: 65, Loss: 2.2603201866149902, Accuracy: 0.384765625\n",
      "Batch: 66, Loss: 2.167088270187378, Accuracy: 0.412109375\n",
      "Batch: 67, Loss: 2.1536126136779785, Accuracy: 0.40625\n",
      "Batch: 68, Loss: 2.2639660835266113, Accuracy: 0.41015625\n",
      "Batch: 69, Loss: 2.2893166542053223, Accuracy: 0.408203125\n",
      "Batch: 70, Loss: 2.324514865875244, Accuracy: 0.3828125\n",
      "Batch: 71, Loss: 2.2052433490753174, Accuracy: 0.3955078125\n",
      "Batch: 72, Loss: 2.1579179763793945, Accuracy: 0.4169921875\n",
      "Batch: 73, Loss: 2.2983179092407227, Accuracy: 0.380859375\n",
      "Batch: 74, Loss: 2.209085464477539, Accuracy: 0.404296875\n",
      "Batch: 75, Loss: 2.118826389312744, Accuracy: 0.4150390625\n",
      "Batch: 76, Loss: 2.1048970222473145, Accuracy: 0.3994140625\n",
      "Batch: 77, Loss: 2.1785318851470947, Accuracy: 0.4091796875\n",
      "Batch: 78, Loss: 2.3164401054382324, Accuracy: 0.3955078125\n",
      "Batch: 79, Loss: 2.2244529724121094, Accuracy: 0.4296875\n",
      "Batch: 80, Loss: 2.0195717811584473, Accuracy: 0.4345703125\n",
      "Batch: 81, Loss: 2.0289201736450195, Accuracy: 0.4228515625\n",
      "Batch: 82, Loss: 2.049755573272705, Accuracy: 0.4228515625\n",
      "Batch: 83, Loss: 2.1301865577697754, Accuracy: 0.41796875\n",
      "Batch: 84, Loss: 2.1758337020874023, Accuracy: 0.423828125\n",
      "Batch: 85, Loss: 2.1037163734436035, Accuracy: 0.4423828125\n",
      "Batch: 86, Loss: 2.1484479904174805, Accuracy: 0.4140625\n",
      "Batch: 87, Loss: 2.132040023803711, Accuracy: 0.4384765625\n",
      "Batch: 88, Loss: 2.1974897384643555, Accuracy: 0.408203125\n",
      "Batch: 89, Loss: 2.173029899597168, Accuracy: 0.41015625\n",
      "Batch: 90, Loss: 2.105435609817505, Accuracy: 0.439453125\n",
      "Batch: 91, Loss: 2.0825982093811035, Accuracy: 0.4228515625\n",
      "Batch: 92, Loss: 2.107478618621826, Accuracy: 0.4296875\n",
      "Batch: 93, Loss: 2.1023690700531006, Accuracy: 0.4228515625\n",
      "Batch: 94, Loss: 2.0590157508850098, Accuracy: 0.4345703125\n",
      "Batch: 95, Loss: 1.9521074295043945, Accuracy: 0.4541015625\n",
      "Batch: 96, Loss: 2.138758897781372, Accuracy: 0.4375\n",
      "Batch: 97, Loss: 2.03304386138916, Accuracy: 0.46484375\n",
      "Batch: 98, Loss: 2.0507349967956543, Accuracy: 0.4716796875\n",
      "Batch: 99, Loss: 1.9657361507415771, Accuracy: 0.451171875\n",
      "Batch: 100, Loss: 1.903275489807129, Accuracy: 0.46484375\n",
      "Batch: 101, Loss: 1.9986332654953003, Accuracy: 0.4423828125\n",
      "Batch: 102, Loss: 1.9451355934143066, Accuracy: 0.44921875\n",
      "Batch: 103, Loss: 2.192152500152588, Accuracy: 0.4189453125\n"
     ]
    }
   ],
   "source": [
    "file = open(os.path.join(data_directory, data_file), mode = 'r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "if __name__ == \"__main__\":\n",
    "    training_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.643317</td>\n",
       "      <td>0.290039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.873376</td>\n",
       "      <td>0.496094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.548782</td>\n",
       "      <td>0.557617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.417467</td>\n",
       "      <td>0.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.348234</td>\n",
       "      <td>0.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.265394</td>\n",
       "      <td>0.618164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.186394</td>\n",
       "      <td>0.630859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.145774</td>\n",
       "      <td>0.642578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.097427</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.073594</td>\n",
       "      <td>0.650391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.052364</td>\n",
       "      <td>0.674805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.011208</td>\n",
       "      <td>0.666016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.004766</td>\n",
       "      <td>0.672852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.980474</td>\n",
       "      <td>0.685547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.972413</td>\n",
       "      <td>0.681641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.935533</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.930730</td>\n",
       "      <td>0.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.896762</td>\n",
       "      <td>0.713867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.896243</td>\n",
       "      <td>0.708008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.886423</td>\n",
       "      <td>0.713867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.853774</td>\n",
       "      <td>0.722656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.858230</td>\n",
       "      <td>0.716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.861040</td>\n",
       "      <td>0.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.835705</td>\n",
       "      <td>0.721680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.818098</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.807396</td>\n",
       "      <td>0.725586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.800719</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.797581</td>\n",
       "      <td>0.740234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.786037</td>\n",
       "      <td>0.737305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.769117</td>\n",
       "      <td>0.744141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.649517</td>\n",
       "      <td>0.786133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.646988</td>\n",
       "      <td>0.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.794922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0.626719</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>0.643305</td>\n",
       "      <td>0.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>0.628394</td>\n",
       "      <td>0.803711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>0.639661</td>\n",
       "      <td>0.797852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>0.620944</td>\n",
       "      <td>0.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>0.593881</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>0.609697</td>\n",
       "      <td>0.798828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>0.592073</td>\n",
       "      <td>0.807617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>0.591695</td>\n",
       "      <td>0.807617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>0.593241</td>\n",
       "      <td>0.805664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>0.592210</td>\n",
       "      <td>0.815430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>0.600351</td>\n",
       "      <td>0.801758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>0.546197</td>\n",
       "      <td>0.823242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.809570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>0.562345</td>\n",
       "      <td>0.805664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>0.562496</td>\n",
       "      <td>0.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>0.558382</td>\n",
       "      <td>0.818359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>0.558365</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>0.576193</td>\n",
       "      <td>0.820312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>0.817383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>0.537521</td>\n",
       "      <td>0.838867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>0.558197</td>\n",
       "      <td>0.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>0.835938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>0.534475</td>\n",
       "      <td>0.825195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>0.515541</td>\n",
       "      <td>0.828125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch      Loss  Accuracy\n",
       "0       1  2.643317  0.290039\n",
       "1       2  1.873376  0.496094\n",
       "2       3  1.548782  0.557617\n",
       "3       4  1.417467  0.597656\n",
       "4       5  1.348234  0.585938\n",
       "5       6  1.265394  0.618164\n",
       "6       7  1.186394  0.630859\n",
       "7       8  1.145774  0.642578\n",
       "8       9  1.097427  0.656250\n",
       "9      10  1.073594  0.650391\n",
       "10     11  1.052364  0.674805\n",
       "11     12  1.011208  0.666016\n",
       "12     13  1.004766  0.672852\n",
       "13     14  0.980474  0.685547\n",
       "14     15  0.972413  0.681641\n",
       "15     16  0.935533  0.703125\n",
       "16     17  0.930730  0.695312\n",
       "17     18  0.896762  0.713867\n",
       "18     19  0.896243  0.708008\n",
       "19     20  0.886423  0.713867\n",
       "20     21  0.853774  0.722656\n",
       "21     22  0.858230  0.716797\n",
       "22     23  0.861040  0.707031\n",
       "23     24  0.835705  0.721680\n",
       "24     25  0.818098  0.735352\n",
       "25     26  0.807396  0.725586\n",
       "26     27  0.800719  0.735352\n",
       "27     28  0.797581  0.740234\n",
       "28     29  0.786037  0.737305\n",
       "29     30  0.769117  0.744141\n",
       "..    ...       ...       ...\n",
       "50     51  0.649517  0.786133\n",
       "51     52  0.646988  0.793945\n",
       "52     53  0.635747  0.794922\n",
       "53     54  0.626719  0.812500\n",
       "54     55  0.643305  0.793945\n",
       "55     56  0.628394  0.803711\n",
       "56     57  0.639661  0.797852\n",
       "57     58  0.620944  0.800781\n",
       "58     59  0.593881  0.812500\n",
       "59     60  0.609697  0.798828\n",
       "60     61  0.592073  0.807617\n",
       "61     62  0.591695  0.807617\n",
       "62     63  0.593241  0.805664\n",
       "63     64  0.592210  0.815430\n",
       "64     65  0.600351  0.801758\n",
       "65     66  0.546197  0.823242\n",
       "66     67  0.582400  0.817383\n",
       "67     68  0.582353  0.817383\n",
       "68     69  0.560872  0.809570\n",
       "69     70  0.562345  0.805664\n",
       "70     71  0.562496  0.828125\n",
       "71     72  0.558382  0.818359\n",
       "72     73  0.558365  0.821289\n",
       "73     74  0.576193  0.820312\n",
       "74     75  0.592619  0.817383\n",
       "75     76  0.537521  0.838867\n",
       "76     77  0.558197  0.821289\n",
       "77     78  0.541944  0.835938\n",
       "78     79  0.534475  0.825195\n",
       "79     80  0.515541  0.828125\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = pd.read_csv(os.path.join(data_directory, \"log.csv\"))\n",
    "log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
